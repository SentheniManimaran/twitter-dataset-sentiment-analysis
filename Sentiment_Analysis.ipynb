{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2742dbba",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afab2776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d4cdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('cleaned_tweets_sentiments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3f7ffe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>social distancing done right</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deepaavali 2020 day 2 thaaimaman house family ...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kluster najib apa sik saman terus sidak semua ...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ju yy that s the unique part of our compoundin...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cherating #socialdistancing</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet Sentiment\n",
       "0                      social distancing done right   POSITIVE\n",
       "1  deepaavali 2020 day 2 thaaimaman house family ...  POSITIVE\n",
       "2  kluster najib apa sik saman terus sidak semua ...  NEGATIVE\n",
       "3  ju yy that s the unique part of our compoundin...   NEUTRAL\n",
       "4                       cherating #socialdistancing    NEUTRAL"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88759185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6999, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a6dd5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6999 entries, 0 to 6998\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Tweet      6999 non-null   object\n",
      " 1   Sentiment  6999 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 109.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "294cc6b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE', 'NEGATIVE', 'NEUTRAL'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Sentiment'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c83e01c",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02115061",
   "metadata": {},
   "source": [
    "### Encode Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9b03846",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode\n",
    "le = preprocessing.LabelEncoder()\n",
    "data['Sentiment'] = le.fit_transform(data['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce1ea618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>social distancing done right</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deepaavali 2020 day 2 thaaimaman house family ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kluster najib apa sik saman terus sidak semua ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ju yy that s the unique part of our compoundin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cherating #socialdistancing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  Sentiment\n",
       "0                      social distancing done right           2\n",
       "1  deepaavali 2020 day 2 thaaimaman house family ...          2\n",
       "2  kluster najib apa sik saman terus sidak semua ...          0\n",
       "3  ju yy that s the unique part of our compoundin...          1\n",
       "4                       cherating #socialdistancing           1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()\n",
    "#0:Negative, 1:Neutral, 2:Positive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bb8c8e",
   "metadata": {},
   "source": [
    "### Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3b42344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6994    sebuah kluster baru dikesan di sebuah restoran...\n",
       "6995                comei je yg social distancing nya pon\n",
       "6996    ya allah kena pergi sekolah kemas harini penat...\n",
       "6997    we open now guys jom ke kita bukak setiap hari...\n",
       "6998    thank you for pointing this out i m thankful f...\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Tweet']=data['Tweet'].str.lower()\n",
    "data['Tweet'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d0fbe2",
   "metadata": {},
   "source": [
    "### Punctuation Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75058cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                        social distancing done right \n",
       "1    deepaavali 2020 day 2 thaaimaman house family ...\n",
       "2    kluster najib apa sik saman terus sidak semua ...\n",
       "3    ju yy that s the unique part of our compoundin...\n",
       "4                          cherating socialdistancing \n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_punctuations = string.punctuation\n",
    "punctuations_list = english_punctuations\n",
    "\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "\n",
    "data['Tweet']= data['Tweet'].apply(lambda x: cleaning_punctuations(x))\n",
    "data['Tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04e5d71",
   "metadata": {},
   "source": [
    "### Stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "285f95c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "english_stop_words = stopwords.words('english')\n",
    "english_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2cb40c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abdul', 'abdullah', 'acara', 'ada', 'adalah', 'ahmad', 'air', 'akan', 'akhbar', 'akhir', 'aktiviti', 'alam', 'amat', 'amerika', 'anak', 'anggota', 'antara', 'antarabangsa', 'apa', 'apabila', 'april', 'as', 'asas', 'asean', 'asia', 'asing', 'atas', 'atau', 'australia', 'awal', 'awam', 'bagaimanapun', 'bagi', 'bahagian', 'bahan', 'baharu', 'bahawa', 'baik', 'bandar', 'bank', 'banyak', 'barangan', 'baru', 'baru-baru', 'bawah', 'beberapa', 'bekas', 'beliau', 'belum', 'berada', 'berakhir', 'berbanding', 'berdasarkan', 'berharap', 'berikutan', 'berjaya', 'berjumlah', 'berkaitan', 'berkata', 'berkenaan', 'berlaku', 'bermula', 'bernama', 'bernilai', 'bersama', 'berubah', 'besar', 'bhd', 'bidang', 'bilion', 'bn', 'boleh', 'bukan', 'bulan', 'bursa', 'cadangan', 'china', 'dagangan', 'dalam', 'dan', 'dana', 'dapat', 'dari', 'daripada', 'dasar', 'datang', 'datuk', 'demikian', 'dengan', 'depan', 'derivatives', 'dewan', 'di', 'diadakan', 'dibuka', 'dicatatkan', 'dijangka', 'diniagakan', 'dis', 'disember', 'ditutup', 'dolar', 'dr', 'dua', 'dunia', 'ekonomi', 'eksekutif', 'eksport', 'empat', 'enam', 'faedah', 'feb', 'global', 'hadapan', 'hanya', 'harga', 'hari', 'hasil', 'hingga', 'hubungan', 'ia', 'iaitu', 'ialah', 'indeks', 'india', 'indonesia', 'industri', 'ini', 'islam', 'isnin', 'isu', 'itu', 'jabatan', 'jalan', 'jan', 'jawatan', 'jawatankuasa', 'jepun', 'jika', 'jualan', 'juga', 'julai', 'jumaat', 'jumlah', 'jun', 'juta', 'kadar', 'kalangan', 'kali', 'kami', 'kata', 'katanya', 'kaunter', 'kawasan', 'ke', 'keadaan', 'kecil', 'kedua', 'kedua-dua', 'kedudukan', 'kekal', 'kementerian', 'kemudahan', 'kenaikan', 'kenyataan', 'kepada', 'kepentingan', 'keputusan', 'kerajaan', 'kerana', 'kereta', 'kerja', 'kerjasama', 'kes', 'keselamatan', 'keseluruhan', 'kesihatan', 'ketika', 'ketua', 'keuntungan', 'kewangan', 'khamis', 'kini', 'kira-kira', 'kita', 'klci', 'klibor', 'komposit', 'kontrak', 'kos', 'kuala', 'kuasa', 'kukuh', 'kumpulan', 'lagi', 'lain', 'langkah', 'laporan', 'lebih', 'lepas', 'lima', 'lot', 'luar', 'lumpur', 'mac', 'mahkamah', 'mahu', 'majlis', 'makanan', 'maklumat', 'malam', 'malaysia', 'mana', 'manakala', 'masa', 'masalah', 'masih', 'masing-masing', 'masyarakat', 'mata', 'media', 'mei', 'melalui', 'melihat', 'memandangkan', 'memastikan', 'membantu', 'membawa', 'memberi', 'memberikan', 'membolehkan', 'membuat', 'mempunyai', 'menambah', 'menarik', 'menawarkan', 'mencapai', 'mencatatkan', 'mendapat', 'mendapatkan', 'menerima', 'menerusi', 'mengadakan', 'mengambil', 'mengenai', 'menggalakkan', 'menggunakan', 'mengikut', 'mengumumkan', 'mengurangkan', 'meningkat', 'meningkatkan', 'menjadi', 'menjelang', 'menokok', 'menteri', 'menunjukkan', 'menurut', 'menyaksikan', 'menyediakan', 'mereka', 'merosot', 'merupakan', 'mesyuarat', 'minat', 'minggu', 'minyak', 'modal', 'mohd', 'mudah', 'mungkin', 'naik', 'najib', 'nasional', 'negara', 'negara-negara', 'negeri', 'niaga', 'nilai', 'nov', 'ogos', 'okt', 'oleh', 'operasi', 'orang', 'pada', 'pagi', 'paling', 'pameran', 'papan', 'para', 'paras', 'parlimen', 'parti', 'pasaran', 'pasukan', 'pegawai', 'pejabat', 'pekerja', 'pelabur', 'pelaburan', 'pelancongan', 'pelanggan', 'pelbagai', 'peluang', 'pembangunan', 'pemberita', 'pembinaan', 'pemimpin', 'pendapatan', 'pendidikan', 'penduduk', 'penerbangan', 'pengarah', 'pengeluaran', 'pengerusi', 'pengguna', 'pengurusan', 'peniaga', 'peningkatan', 'penting', 'peratus', 'perdagangan', 'perdana', 'peringkat', 'perjanjian', 'perkara', 'perkhidmatan', 'perladangan', 'perlu', 'permintaan', 'perniagaan', 'persekutuan', 'persidangan', 'pertama', 'pertubuhan', 'pertumbuhan', 'perusahaan', 'peserta', 'petang', 'pihak', 'pilihan', 'pinjaman', 'polis', 'politik', 'presiden', 'prestasi', 'produk', 'program', 'projek', 'proses', 'proton', 'pukul', 'pula', 'pusat', 'rabu', 'rakan', 'rakyat', 'ramai', 'rantau', 'raya', 'rendah', 'ringgit', 'rumah', 'sabah', 'sahaja', 'saham', 'sama', 'sarawak', 'satu', 'sawit', 'saya', 'sdn', 'sebagai', 'sebahagian', 'sebanyak', 'sebarang', 'sebelum', 'sebelumnya', 'sebuah', 'secara', 'sedang', 'segi', 'sehingga', 'sejak', 'sekarang', 'sektor', 'sekuriti', 'selain', 'selama', 'selasa', 'selatan', 'selepas', 'seluruh', 'semakin', 'semalam', 'semasa', 'sementara', 'semua', 'semula', 'sen', 'sendiri', 'seorang', 'sepanjang', 'seperti', 'sept', 'september', 'serantau', 'seri', 'serta', 'sesi', 'setiap', 'setiausaha', 'sidang', 'singapura', 'sini', 'sistem', 'sokongan', 'sri', 'sudah', 'sukan', 'suku', 'sumber', 'supaya', 'susut', 'syarikat', 'syed', 'tahap', 'tahun', 'tan', 'tanah', 'tanpa', 'tawaran', 'teknologi', 'telah', 'tempat', 'tempatan', 'tempoh', 'tenaga', 'tengah', 'tentang', 'terbaik', 'terbang', 'terbesar', 'terbuka', 'terdapat', 'terhadap', 'termasuk', 'tersebut', 'terus', 'tetapi', 'thailand', 'tiada', 'tidak', 'tiga', 'timbalan', 'timur', 'tindakan', 'tinggi', 'tun', 'tunai', 'turun', 'turut', 'umno', 'unit', 'untuk', 'untung', 'urus', 'usaha', 'utama', 'walaupun', 'wang', 'wanita', 'wilayah', 'yang']\n"
     ]
    }
   ],
   "source": [
    "malay_stop_words = []\n",
    "\n",
    "with open('stopwords-ms.txt', \"r\") as file:\n",
    "    for line in file:\n",
    "        stop_word = line.strip()\n",
    "        malay_stop_words.append(stop_word)\n",
    "\n",
    "print(malay_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e6913eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'abdul',\n",
       " 'abdullah',\n",
       " 'acara',\n",
       " 'ada',\n",
       " 'adalah',\n",
       " 'ahmad',\n",
       " 'air',\n",
       " 'akan',\n",
       " 'akhbar',\n",
       " 'akhir',\n",
       " 'aktiviti',\n",
       " 'alam',\n",
       " 'amat',\n",
       " 'amerika',\n",
       " 'anak',\n",
       " 'anggota',\n",
       " 'antara',\n",
       " 'antarabangsa',\n",
       " 'apa',\n",
       " 'apabila',\n",
       " 'april',\n",
       " 'as',\n",
       " 'asas',\n",
       " 'asean',\n",
       " 'asia',\n",
       " 'asing',\n",
       " 'atas',\n",
       " 'atau',\n",
       " 'australia',\n",
       " 'awal',\n",
       " 'awam',\n",
       " 'bagaimanapun',\n",
       " 'bagi',\n",
       " 'bahagian',\n",
       " 'bahan',\n",
       " 'baharu',\n",
       " 'bahawa',\n",
       " 'baik',\n",
       " 'bandar',\n",
       " 'bank',\n",
       " 'banyak',\n",
       " 'barangan',\n",
       " 'baru',\n",
       " 'baru-baru',\n",
       " 'bawah',\n",
       " 'beberapa',\n",
       " 'bekas',\n",
       " 'beliau',\n",
       " 'belum',\n",
       " 'berada',\n",
       " 'berakhir',\n",
       " 'berbanding',\n",
       " 'berdasarkan',\n",
       " 'berharap',\n",
       " 'berikutan',\n",
       " 'berjaya',\n",
       " 'berjumlah',\n",
       " 'berkaitan',\n",
       " 'berkata',\n",
       " 'berkenaan',\n",
       " 'berlaku',\n",
       " 'bermula',\n",
       " 'bernama',\n",
       " 'bernilai',\n",
       " 'bersama',\n",
       " 'berubah',\n",
       " 'besar',\n",
       " 'bhd',\n",
       " 'bidang',\n",
       " 'bilion',\n",
       " 'bn',\n",
       " 'boleh',\n",
       " 'bukan',\n",
       " 'bulan',\n",
       " 'bursa',\n",
       " 'cadangan',\n",
       " 'china',\n",
       " 'dagangan',\n",
       " 'dalam',\n",
       " 'dan',\n",
       " 'dana',\n",
       " 'dapat',\n",
       " 'dari',\n",
       " 'daripada',\n",
       " 'dasar',\n",
       " 'datang',\n",
       " 'datuk',\n",
       " 'demikian',\n",
       " 'dengan',\n",
       " 'depan',\n",
       " 'derivatives',\n",
       " 'dewan',\n",
       " 'di',\n",
       " 'diadakan',\n",
       " 'dibuka',\n",
       " 'dicatatkan',\n",
       " 'dijangka',\n",
       " 'diniagakan',\n",
       " 'dis',\n",
       " 'disember',\n",
       " 'ditutup',\n",
       " 'dolar',\n",
       " 'dr',\n",
       " 'dua',\n",
       " 'dunia',\n",
       " 'ekonomi',\n",
       " 'eksekutif',\n",
       " 'eksport',\n",
       " 'empat',\n",
       " 'enam',\n",
       " 'faedah',\n",
       " 'feb',\n",
       " 'global',\n",
       " 'hadapan',\n",
       " 'hanya',\n",
       " 'harga',\n",
       " 'hari',\n",
       " 'hasil',\n",
       " 'hingga',\n",
       " 'hubungan',\n",
       " 'ia',\n",
       " 'iaitu',\n",
       " 'ialah',\n",
       " 'indeks',\n",
       " 'india',\n",
       " 'indonesia',\n",
       " 'industri',\n",
       " 'ini',\n",
       " 'islam',\n",
       " 'isnin',\n",
       " 'isu',\n",
       " 'itu',\n",
       " 'jabatan',\n",
       " 'jalan',\n",
       " 'jan',\n",
       " 'jawatan',\n",
       " 'jawatankuasa',\n",
       " 'jepun',\n",
       " 'jika',\n",
       " 'jualan',\n",
       " 'juga',\n",
       " 'julai',\n",
       " 'jumaat',\n",
       " 'jumlah',\n",
       " 'jun',\n",
       " 'juta',\n",
       " 'kadar',\n",
       " 'kalangan',\n",
       " 'kali',\n",
       " 'kami',\n",
       " 'kata',\n",
       " 'katanya',\n",
       " 'kaunter',\n",
       " 'kawasan',\n",
       " 'ke',\n",
       " 'keadaan',\n",
       " 'kecil',\n",
       " 'kedua',\n",
       " 'kedua-dua',\n",
       " 'kedudukan',\n",
       " 'kekal',\n",
       " 'kementerian',\n",
       " 'kemudahan',\n",
       " 'kenaikan',\n",
       " 'kenyataan',\n",
       " 'kepada',\n",
       " 'kepentingan',\n",
       " 'keputusan',\n",
       " 'kerajaan',\n",
       " 'kerana',\n",
       " 'kereta',\n",
       " 'kerja',\n",
       " 'kerjasama',\n",
       " 'kes',\n",
       " 'keselamatan',\n",
       " 'keseluruhan',\n",
       " 'kesihatan',\n",
       " 'ketika',\n",
       " 'ketua',\n",
       " 'keuntungan',\n",
       " 'kewangan',\n",
       " 'khamis',\n",
       " 'kini',\n",
       " 'kira-kira',\n",
       " 'kita',\n",
       " 'klci',\n",
       " 'klibor',\n",
       " 'komposit',\n",
       " 'kontrak',\n",
       " 'kos',\n",
       " 'kuala',\n",
       " 'kuasa',\n",
       " 'kukuh',\n",
       " 'kumpulan',\n",
       " 'lagi',\n",
       " 'lain',\n",
       " 'langkah',\n",
       " 'laporan',\n",
       " 'lebih',\n",
       " 'lepas',\n",
       " 'lima',\n",
       " 'lot',\n",
       " 'luar',\n",
       " 'lumpur',\n",
       " 'mac',\n",
       " 'mahkamah',\n",
       " 'mahu',\n",
       " 'majlis',\n",
       " 'makanan',\n",
       " 'maklumat',\n",
       " 'malam',\n",
       " 'malaysia',\n",
       " 'mana',\n",
       " 'manakala',\n",
       " 'masa',\n",
       " 'masalah',\n",
       " 'masih',\n",
       " 'masing-masing',\n",
       " 'masyarakat',\n",
       " 'mata',\n",
       " 'media',\n",
       " 'mei',\n",
       " 'melalui',\n",
       " 'melihat',\n",
       " 'memandangkan',\n",
       " 'memastikan',\n",
       " 'membantu',\n",
       " 'membawa',\n",
       " 'memberi',\n",
       " 'memberikan',\n",
       " 'membolehkan',\n",
       " 'membuat',\n",
       " 'mempunyai',\n",
       " 'menambah',\n",
       " 'menarik',\n",
       " 'menawarkan',\n",
       " 'mencapai',\n",
       " 'mencatatkan',\n",
       " 'mendapat',\n",
       " 'mendapatkan',\n",
       " 'menerima',\n",
       " 'menerusi',\n",
       " 'mengadakan',\n",
       " 'mengambil',\n",
       " 'mengenai',\n",
       " 'menggalakkan',\n",
       " 'menggunakan',\n",
       " 'mengikut',\n",
       " 'mengumumkan',\n",
       " 'mengurangkan',\n",
       " 'meningkat',\n",
       " 'meningkatkan',\n",
       " 'menjadi',\n",
       " 'menjelang',\n",
       " 'menokok',\n",
       " 'menteri',\n",
       " 'menunjukkan',\n",
       " 'menurut',\n",
       " 'menyaksikan',\n",
       " 'menyediakan',\n",
       " 'mereka',\n",
       " 'merosot',\n",
       " 'merupakan',\n",
       " 'mesyuarat',\n",
       " 'minat',\n",
       " 'minggu',\n",
       " 'minyak',\n",
       " 'modal',\n",
       " 'mohd',\n",
       " 'mudah',\n",
       " 'mungkin',\n",
       " 'naik',\n",
       " 'najib',\n",
       " 'nasional',\n",
       " 'negara',\n",
       " 'negara-negara',\n",
       " 'negeri',\n",
       " 'niaga',\n",
       " 'nilai',\n",
       " 'nov',\n",
       " 'ogos',\n",
       " 'okt',\n",
       " 'oleh',\n",
       " 'operasi',\n",
       " 'orang',\n",
       " 'pada',\n",
       " 'pagi',\n",
       " 'paling',\n",
       " 'pameran',\n",
       " 'papan',\n",
       " 'para',\n",
       " 'paras',\n",
       " 'parlimen',\n",
       " 'parti',\n",
       " 'pasaran',\n",
       " 'pasukan',\n",
       " 'pegawai',\n",
       " 'pejabat',\n",
       " 'pekerja',\n",
       " 'pelabur',\n",
       " 'pelaburan',\n",
       " 'pelancongan',\n",
       " 'pelanggan',\n",
       " 'pelbagai',\n",
       " 'peluang',\n",
       " 'pembangunan',\n",
       " 'pemberita',\n",
       " 'pembinaan',\n",
       " 'pemimpin',\n",
       " 'pendapatan',\n",
       " 'pendidikan',\n",
       " 'penduduk',\n",
       " 'penerbangan',\n",
       " 'pengarah',\n",
       " 'pengeluaran',\n",
       " 'pengerusi',\n",
       " 'pengguna',\n",
       " 'pengurusan',\n",
       " 'peniaga',\n",
       " 'peningkatan',\n",
       " 'penting',\n",
       " 'peratus',\n",
       " 'perdagangan',\n",
       " 'perdana',\n",
       " 'peringkat',\n",
       " 'perjanjian',\n",
       " 'perkara',\n",
       " 'perkhidmatan',\n",
       " 'perladangan',\n",
       " 'perlu',\n",
       " 'permintaan',\n",
       " 'perniagaan',\n",
       " 'persekutuan',\n",
       " 'persidangan',\n",
       " 'pertama',\n",
       " 'pertubuhan',\n",
       " 'pertumbuhan',\n",
       " 'perusahaan',\n",
       " 'peserta',\n",
       " 'petang',\n",
       " 'pihak',\n",
       " 'pilihan',\n",
       " 'pinjaman',\n",
       " 'polis',\n",
       " 'politik',\n",
       " 'presiden',\n",
       " 'prestasi',\n",
       " 'produk',\n",
       " 'program',\n",
       " 'projek',\n",
       " 'proses',\n",
       " 'proton',\n",
       " 'pukul',\n",
       " 'pula',\n",
       " 'pusat',\n",
       " 'rabu',\n",
       " 'rakan',\n",
       " 'rakyat',\n",
       " 'ramai',\n",
       " 'rantau',\n",
       " 'raya',\n",
       " 'rendah',\n",
       " 'ringgit',\n",
       " 'rumah',\n",
       " 'sabah',\n",
       " 'sahaja',\n",
       " 'saham',\n",
       " 'sama',\n",
       " 'sarawak',\n",
       " 'satu',\n",
       " 'sawit',\n",
       " 'saya',\n",
       " 'sdn',\n",
       " 'sebagai',\n",
       " 'sebahagian',\n",
       " 'sebanyak',\n",
       " 'sebarang',\n",
       " 'sebelum',\n",
       " 'sebelumnya',\n",
       " 'sebuah',\n",
       " 'secara',\n",
       " 'sedang',\n",
       " 'segi',\n",
       " 'sehingga',\n",
       " 'sejak',\n",
       " 'sekarang',\n",
       " 'sektor',\n",
       " 'sekuriti',\n",
       " 'selain',\n",
       " 'selama',\n",
       " 'selasa',\n",
       " 'selatan',\n",
       " 'selepas',\n",
       " 'seluruh',\n",
       " 'semakin',\n",
       " 'semalam',\n",
       " 'semasa',\n",
       " 'sementara',\n",
       " 'semua',\n",
       " 'semula',\n",
       " 'sen',\n",
       " 'sendiri',\n",
       " 'seorang',\n",
       " 'sepanjang',\n",
       " 'seperti',\n",
       " 'sept',\n",
       " 'september',\n",
       " 'serantau',\n",
       " 'seri',\n",
       " 'serta',\n",
       " 'sesi',\n",
       " 'setiap',\n",
       " 'setiausaha',\n",
       " 'sidang',\n",
       " 'singapura',\n",
       " 'sini',\n",
       " 'sistem',\n",
       " 'sokongan',\n",
       " 'sri',\n",
       " 'sudah',\n",
       " 'sukan',\n",
       " 'suku',\n",
       " 'sumber',\n",
       " 'supaya',\n",
       " 'susut',\n",
       " 'syarikat',\n",
       " 'syed',\n",
       " 'tahap',\n",
       " 'tahun',\n",
       " 'tan',\n",
       " 'tanah',\n",
       " 'tanpa',\n",
       " 'tawaran',\n",
       " 'teknologi',\n",
       " 'telah',\n",
       " 'tempat',\n",
       " 'tempatan',\n",
       " 'tempoh',\n",
       " 'tenaga',\n",
       " 'tengah',\n",
       " 'tentang',\n",
       " 'terbaik',\n",
       " 'terbang',\n",
       " 'terbesar',\n",
       " 'terbuka',\n",
       " 'terdapat',\n",
       " 'terhadap',\n",
       " 'termasuk',\n",
       " 'tersebut',\n",
       " 'terus',\n",
       " 'tetapi',\n",
       " 'thailand',\n",
       " 'tiada',\n",
       " 'tidak',\n",
       " 'tiga',\n",
       " 'timbalan',\n",
       " 'timur',\n",
       " 'tindakan',\n",
       " 'tinggi',\n",
       " 'tun',\n",
       " 'tunai',\n",
       " 'turun',\n",
       " 'turut',\n",
       " 'umno',\n",
       " 'unit',\n",
       " 'untuk',\n",
       " 'untung',\n",
       " 'urus',\n",
       " 'usaha',\n",
       " 'utama',\n",
       " 'walaupun',\n",
       " 'wang',\n",
       " 'wanita',\n",
       " 'wilayah',\n",
       " 'yang',\n",
       " 'mco',\n",
       " 'sosial',\n",
       " 'penjarakan',\n",
       " 'socialdistancing',\n",
       " 'covid19',\n",
       " 'kitajagakita',\n",
       " 'stayathome',\n",
       " 'staysafe',\n",
       " 'covid',\n",
       " 'penjarakansosial',\n",
       " 'social',\n",
       " 'distancing']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_hashtags = ['mco','sosial','penjarakan','socialdistancing', 'covid19', 'kitajagakita', 'stayathome', 'staysafe', 'covid', 'penjarakansosial','social','distancing']\n",
    "stop_words = english_stop_words + malay_stop_words + frequent_hashtags\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f5ca842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>done right</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deepaavali 2020 day 2 thaaimaman house family ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kluster sik saman sidak sia sikda gk https</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ju yy unique part compounding lab fits sop</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cherating</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  Sentiment\n",
       "0                                         done right          2\n",
       "1  deepaavali 2020 day 2 thaaimaman house family ...          2\n",
       "2         kluster sik saman sidak sia sikda gk https          0\n",
       "3         ju yy unique part compounding lab fits sop          1\n",
       "4                                          cherating          1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOPWORDS = set(stop_words)\n",
    "\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "data['Tweet'] = data['Tweet'].apply(lambda text: cleaning_stopwords(text))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b264da9",
   "metadata": {},
   "source": [
    "### Clean Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56d82817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                           done right\n",
       "1    deepaavali  day  thaaimaman house family dinne...\n",
       "2           kluster sik saman sidak sia sikda gk https\n",
       "3           ju yy unique part compounding lab fits sop\n",
       "4                                            cherating\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleaning_numbers(data):\n",
    "    return re.sub('[0-9]+', '', data)\n",
    "\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: cleaning_numbers(x))\n",
    "data['Tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "834f4ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleansed_negative = data[data['Sentiment'] == 0]\n",
    "cleansed_neutral = data[data['Sentiment'] == 1]\n",
    "cleansed_positive = data[data['Sentiment'] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202fae98",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ebe01c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                        [done, right]\n",
       "1    [deepaavali, day, thaaimaman, house, family, d...\n",
       "2    [kluster, sik, saman, sidak, sia, sikda, gk, h...\n",
       "3    [ju, yy, unique, part, compounding, lab, fits,...\n",
       "4                                          [cherating]\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "data['Tweet'] = data['Tweet'].apply(tokenizer.tokenize)\n",
    "data['Tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a3ef90",
   "metadata": {},
   "source": [
    "### Separating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ce3b8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['Tweet']\n",
    "y = data['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d450a713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                           [done, right]\n",
      "1       [deepaavali, day, thaaimaman, house, family, d...\n",
      "2       [kluster, sik, saman, sidak, sia, sikda, gk, h...\n",
      "3       [ju, yy, unique, part, compounding, lab, fits,...\n",
      "4                                             [cherating]\n",
      "                              ...                        \n",
      "6994    [kluster, dikesan, restoran, brickfields, sari...\n",
      "6995                            [comei, je, yg, nya, pon]\n",
      "6996    [ya, allah, kena, pergi, sekolah, kemas, harin...\n",
      "6997    [open, guys, jom, bukak, jam, pm, nasi, kerabu...\n",
      "6998    [thank, pointing, thankful, whatever, returned...\n",
      "Name: Tweet, Length: 6999, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "451d8761",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.20, random_state =42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3e664e",
   "metadata": {},
   "source": [
    "### Text Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c037e663",
   "metadata": {},
   "source": [
    "##### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8d3bd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_X_train_tf = [\" \".join(doc) for doc in X_train]\n",
    "documents_X_test_tf = [\" \".join(doc) for doc in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2598d48c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of feature_words:  66616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "tf_vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
    "tfidf_matrix = tf_vectorizer.fit_transform(documents_X_train_tf)\n",
    "print('No. of feature_words: ', len(tf_vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa748062",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tf  = tf_vectorizer.transform(documents_X_test_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8ec4aa",
   "metadata": {},
   "source": [
    "##### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e1967e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_X_train_w2v = [\" \".join(doc) for doc in X_train]\n",
    "documents_X_test_w2v = [\" \".join(doc) for doc in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "011121be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(documents_X_train_w2v, min_count=1, vector_size=100, window=5, sg=1)\n",
    "\n",
    "def calculate_document_vectors(documents, model):\n",
    "    document_features = []\n",
    "    for doc in documents:\n",
    "        tokens = doc.split()\n",
    "        valid_tokens = [word for word in tokens if word in model.wv]\n",
    "        if valid_tokens:\n",
    "            doc_vector = np.mean([model.wv[word] for word in valid_tokens], axis=0)\n",
    "        else:\n",
    "            doc_vector = np.zeros(model.vector_size)\n",
    "        document_features.append(doc_vector)\n",
    "    return document_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c898e0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v = calculate_document_vectors(documents_X_train_w2v, model)\n",
    "X_test_w2v = calculate_document_vectors(documents_X_test_w2v, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b208ca10",
   "metadata": {},
   "source": [
    "##### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04cf1bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_X_train_cv = [\" \".join(doc) for doc in X_train]\n",
    "documents_X_test_cv = [\" \".join(doc) for doc in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0c70af50",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69019d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cv = cv_vectorizer.fit_transform(documents_X_train_cv)\n",
    "X_test_cv = cv_vectorizer.transform(documents_X_test_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4323e34",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "80fc87b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_Evaluate(model):\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    categories = ['Negative', 'Neutral', 'Positive']\n",
    "    group_names = ['TN', 'FP', 'FN', 'T']\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
    "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names, group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2, 2)\n",
    "    sns.heatmap(cf_matrix, annot=labels, cmap='Blues', fmt='',\n",
    "                xticklabels=categories, yticklabels=categories)\n",
    "    plt.xlabel(\"Predicted values\", fontdict={'size': 14}, labelpad=10)\n",
    "    plt.ylabel(\"Actual values\", fontdict={'size': 14}, labelpad=10)\n",
    "    plt.title(\"Confusion Matrix\", fontdict={'size': 18}, pad=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db18ae2",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "00b3bc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[421 101  36]\n",
      " [ 46 291  39]\n",
      " [143 100 223]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.75      0.72       558\n",
      "           1       0.59      0.77      0.67       376\n",
      "           2       0.75      0.48      0.58       466\n",
      "\n",
      "    accuracy                           0.67      1400\n",
      "   macro avg       0.68      0.67      0.66      1400\n",
      "weighted avg       0.68      0.67      0.66      1400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(tfidf_matrix, y_train)\n",
    "predictions_2 = rf.predict(X_test_tf)\n",
    "\n",
    "confusion_mat = confusion_matrix(y_test, predictions_2)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "\n",
    "class_report = classification_report(y_test, predictions_2)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2cfbc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[442  67  49]\n",
      " [ 66 230  80]\n",
      " [140  59 267]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.79      0.73       558\n",
      "           1       0.65      0.61      0.63       376\n",
      "           2       0.67      0.57      0.62       466\n",
      "\n",
      "    accuracy                           0.67      1400\n",
      "   macro avg       0.67      0.66      0.66      1400\n",
      "weighted avg       0.67      0.67      0.67      1400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ls = LinearSVC()\n",
    "ls.fit(tfidf_matrix, y_train)\n",
    "predictions_3 = ls.predict(X_test_tf)\n",
    "\n",
    "confusion_mat = confusion_matrix(y_test, predictions_3)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "\n",
    "class_report = classification_report(y_test, predictions_3)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bd5d8622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[443  69  46]\n",
      " [ 69 226  81]\n",
      " [159  62 245]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.79      0.72       558\n",
      "           1       0.63      0.60      0.62       376\n",
      "           2       0.66      0.53      0.58       466\n",
      "\n",
      "    accuracy                           0.65      1400\n",
      "   macro avg       0.65      0.64      0.64      1400\n",
      "weighted avg       0.65      0.65      0.65      1400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(tfidf_matrix, y_train)\n",
    "predictions_5 = lr.predict(X_test_tf)\n",
    "\n",
    "confusion_mat = confusion_matrix(y_test, predictions_5)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "\n",
    "class_report = classification_report(y_test, predictions_5)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cfeb1d",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1a1b818e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[548   1   9]\n",
      " [365   4   7]\n",
      " [449   2  15]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.98      0.57       558\n",
      "           1       0.57      0.01      0.02       376\n",
      "           2       0.48      0.03      0.06       466\n",
      "\n",
      "    accuracy                           0.41      1400\n",
      "   macro avg       0.49      0.34      0.22      1400\n",
      "weighted avg       0.47      0.41      0.25      1400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_w2v, y_train)\n",
    "predictions_2 = rf.predict(X_test_w2v)\n",
    "\n",
    "confusion_mat = confusion_matrix(y_test, predictions_2)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "\n",
    "class_report = classification_report(y_test, predictions_2)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fb746857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[550   1   7]\n",
      " [365   3   8]\n",
      " [450   1  15]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.99      0.57       558\n",
      "           1       0.60      0.01      0.02       376\n",
      "           2       0.50      0.03      0.06       466\n",
      "\n",
      "    accuracy                           0.41      1400\n",
      "   macro avg       0.50      0.34      0.22      1400\n",
      "weighted avg       0.49      0.41      0.25      1400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ls = LinearSVC()\n",
    "ls.fit(X_train_w2v, y_train)\n",
    "predictions_3 = ls.predict(X_test_w2v)\n",
    "\n",
    "confusion_mat = confusion_matrix(y_test, predictions_3)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "\n",
    "class_report = classification_report(y_test, predictions_3)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6e4ba718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[549   1   8]\n",
      " [365   1  10]\n",
      " [450   0  16]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.98      0.57       558\n",
      "           1       0.50      0.00      0.01       376\n",
      "           2       0.47      0.03      0.06       466\n",
      "\n",
      "    accuracy                           0.40      1400\n",
      "   macro avg       0.46      0.34      0.21      1400\n",
      "weighted avg       0.45      0.40      0.25      1400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_w2v, y_train)\n",
    "predictions_5 = lr.predict(X_test_w2v)\n",
    "\n",
    "confusion_mat = confusion_matrix(y_test, predictions_5)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "\n",
    "class_report = classification_report(y_test, predictions_5)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b995b8",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e0cc2ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[421  95  42]\n",
      " [ 39 283  54]\n",
      " [140  74 252]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.75      0.73       558\n",
      "           1       0.63      0.75      0.68       376\n",
      "           2       0.72      0.54      0.62       466\n",
      "\n",
      "    accuracy                           0.68      1400\n",
      "   macro avg       0.68      0.68      0.68      1400\n",
      "weighted avg       0.69      0.68      0.68      1400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_count = RandomForestClassifier()\n",
    "rf_count.fit(X_train_cv, y_train)\n",
    "predictions_2 = rf_count.predict(X_test_cv)\n",
    "\n",
    "confusion_mat = confusion_matrix(y_test, predictions_2)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "\n",
    "class_report = classification_report(y_test, predictions_2)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "296ca5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[388  83  87]\n",
      " [ 56 256  64]\n",
      " [108  73 285]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.70      0.70       558\n",
      "           1       0.62      0.68      0.65       376\n",
      "           2       0.65      0.61      0.63       466\n",
      "\n",
      "    accuracy                           0.66      1400\n",
      "   macro avg       0.66      0.66      0.66      1400\n",
      "weighted avg       0.66      0.66      0.66      1400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ls = LinearSVC()\n",
    "ls.fit(X_train_cv, y_train)\n",
    "predictions_3 = ls.predict(X_test_cv)\n",
    "\n",
    "confusion_mat = confusion_matrix(y_test, predictions_3)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "\n",
    "class_report = classification_report(y_test, predictions_3)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d6528f17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[405  88  65]\n",
      " [ 53 266  57]\n",
      " [110  80 276]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.73      0.72       558\n",
      "           1       0.61      0.71      0.66       376\n",
      "           2       0.69      0.59      0.64       466\n",
      "\n",
      "    accuracy                           0.68      1400\n",
      "   macro avg       0.67      0.68      0.67      1400\n",
      "weighted avg       0.68      0.68      0.68      1400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_cv, y_train)\n",
    "predictions_5 = lr.predict(X_test_cv)\n",
    "\n",
    "confusion_mat = confusion_matrix(y_test, predictions_5)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "\n",
    "class_report = classification_report(y_test, predictions_5)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d879a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('model.pkl', 'wb') as model_file:\n",
    "#     pickle.dump(rf_count, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b81fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('count_vectorizer.pkl', 'wb') as vectorizer_file:\n",
    "#     pickle.dump(cv_vectorizer, vectorizer_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
